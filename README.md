# Offline RL

This README documents how to get up and running with `d3rlpy` for running offline RL experiments.

## Setting up the environment

We will use Mamba for environment management. If you are using a machine that does not have a GPU, then move to **Non-GPU Environment** for the relevant instructions.

### Installing Mamba

The following instructions are geared towards a Linux system (or WSL). Note that you can also use the Micro Mamba manager if you are short on disk space. Firstly, go to the official [Github repo](https://github.com/conda-forge/miniforge#mambaforge) and download the relevant installer. Generally the one without PyPy should be sufficient. You can also use `curl` or `wget`.

```bash
curl -L -O "https://github.com/conda-forge/miniforge/releases/latest/download/Mambaforge-$(uname)-$(uname -m).sh"
```

Run the install script.

```bash
bash Mambaforge-Linux-x86_64.sh
```

Accepting the defaults is fine here. Restart your shell and check for a successful install with `mamba --version`.

### Cloning the repository

Clone the repository as usual.

```bash
git clone https://github.com/nickmarks00/Need-4-Speed.git
```

Now run the following to create the offline Rl branch locally and set it up to track the remote.

```bash
git branch -f offline-rl origin/offline-rl
```

---

The final step is to create the Mamba environment from file.

```bash
mamba env create --file env.yaml
```

Note that this requires you to have a system running with `cuda` installed. Otherwise it will error when it tries to install the CUDA-compiled version of `PyTorch`. If need be, I can create a separate environment file for CPU-only machines.

The new environment can now be activated.

```bash
mamba activate need4speed
```

### Non-GPU Environment
If you have a machine with no GPU, then the above steps will not work because it expects PyTorch to be compiled with CUDA support. Instead, we can use Pipenv to manage the virtual environment and install the relevant pacakges. First, make sure that Pipenv is installed:

```bash
pip install pipenv
```
Assuming you have cloned the repository and navigated into that directory as above, then simply run

```bash
pipenv install
```

This will automatically install the necessary pacakges and their dependencies. A few comments on Pipenv usage that differ from Mamba. You need to activate the environment by running the command `pipenv shell` when within the repository. Further, to run scripts, you need to prefix these commands with `pipenv run ...` for example

```bash
pipenv run python3 operate.py
```

Note also that depending on your linter your editor may complain about not being able to resolve particular package imports like **numpy** - don't worry, the scripts will still work properly if your environment is activated. There are ways to remove these errors by specifying the python/pip binary that your linter should use for that particular repository.

### Altering the Penguin Pi code
To date, our reward function in training has exploited the horizontal, vertical and angular displacement of the robot across time steps to assist in velocity and pose smoothing. However, this functionality will not work out of the box. It relies on the `get_pose` method defined in `utils/penguin_pi.py`:

```python
def get_pose(self):
        resp = requests.get("{}/robot/get/pose".format(self.endpoint), timeout=1)
        assert resp.status_code == 200
        x, y, theta = list(
            map(float, resp.text.split(","))
        )  # read str,str,str into float,float,float
        return x, y, theta
```

By default, this `{}/robot/get/pose` endpoint is not exposed by the Penguin Pi. To change this, you need to connect to the Pi's hotspot then SSH into the Pi itself:

```bash
ssh pi@<IP>
```

Where `<IP>` is the IP address of the Pi's local hotspot (what is shown on the LCD screen). The default password is `PenguinPi`. Open up the following file:

```bash
vi /usr/bin/penguin-webserver
```
Then add the following code somewhere in the file:

```python
@app.route('/robot/get/pose')
def getpose():
        global x, y, theta
        reutrn "{},{},{}".format(x, y, theta)
```

You should now be able to collect pose information during operation.

## Usage

### Demos

With the environment activated, you are now in a position to train experiments. To test everything is working well, try solving the `cartpole` experiment using a DQN.

```bash
cd demos && python3 cartpole_dqn.py
```

The training should not take long and the logs and data from the experiments should be in an appropriately named folder in `d3rly_data` and `d3rlpy_logs`. There are two other small demos you can use as well to ensure that everything is set up correctly.

### Creating an experiment with `d3rlpy`

Every experiment consists of a few basic blocks: the dataset and environment, the model and the call to train.

1. Datasets and environments are generated by importing the particular experiment of choice from `d3rlpy.datasets`. [Here](https://d3rlpy.readthedocs.io/en/latest/references/datasets.html) is a list of the available ones. Note that the `env` returned from instantiating a game with

```python
dataset, env = get_d4rl("hopper-medium-v2")
```

is an instance of an OpenGym environment and can be used in the same way consequently.

2. A model is created by importing it from the [list](https://d3rlpy.readthedocs.io/en/latest/references/algos.html) of available algorithms and configuring it as desired. If you are training with GPU, be sure to included `device=cuda:0` in the model instantiation.

3. For training, you can add loss and reward evaluators in (the latter only if you still have access to the environment during training), both of which are imported from `d3rlpy.metrics` and example configurations [here](https://d3rlpy.readthedocs.io/en/latest/tutorials/getting_started.html#setup-metrics). Then we call `fit` on the model we created. Everything from the learning rate to the number of epochs can be controlled here.

```python
iql.fit(
        dataset,
        n_steps=10000,
        n_steps_per_epoch=1000,
        save_interval=5,
        experiment_name=experiment_name,
        with_timestamp=False,
        evaluators = {
            'td_error': td_error_evaulator,
            'environment': env_evaluator
            }
)
```

Once the experiment has run, you can plot the results (here, reward) using `d3rlpy plot`:

```bash
d3rlpy plot <path to .csv file>
```

### Collect-Train-Deploy

Use the `operate.py` script to collect expert trajectories. A few comments on this script:
- By default, it will run in continuous mode, which means the following:
  - If you stop the script, then restart it, the program will _append_ to the images folder and actions file rather than overwriting.
  - It will not calculate the rewards at each time step. This will instead be done during training. This also means that it will not produce a live reward plot during operation.
- To not have this be the behaviour, run the script with the argument `--one_shot=True`. This is commonly used to debug the reward function rather than to collect a dataset of trajectories for training. It will have a noticeable impact on the rate at which the Pi captures images - however, you could explore some simple multi-threading to have this not be the case.

Use `train.py` to train a model.
- By default, the script will expect to find all the data it needs in the `output/` directory.
- If it doesn't find rewards calculated, it will do so prior to training.
- There are also a few cleaning things it does, such as removing erroneously large actions from the CSV file and deleting the corresponding images.

Once a model has been trained with `train.py`, it can be deployed to the Penguin Pi using `deploy.py`. To connect to the Penguin Pi:

1. Ensure that it is turned on. Wait until the LCD screen displays a valid WLAN address.
2. Connect to the Penguin Pi's local hotspot. The network name will be something like `penguinpi:xx:xx:xx` where `x` represents some hexadecimal value. The password is `egb439123`.
3. Run `python3 deploy.py`.
4. If you have connection issues, ensure that the IP address set in `deploy.py` parser arguments matches that displayed on the Pi's LCD screen, and the port is correctly set to `8080`. You can also test the connection by opening a browser and navigating to `http://192.168.50.1:8080` - it should present you with a control interface for the Penguin Pi.

Note that by default this script will look in the `models/` directory and use the most recently created/added `.d3` file as the deployment model.
